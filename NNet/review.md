# 神经网络 

## 第一讲

+ 对脑的研究可以促进人工神经网络的研究，对人工神经网络的研究,也可以反过来促进对大脑的研究
+ 意识是人脑对大脑内外表象的觉察，人的头脑对于客观物质世界的反映,也是感觉、思维等各种心理过程的总和
+ 大脑是如何工作的
  + 颅相学解释：不同的大脑功能存在于分立的脑区
  + 聚集场理论：大脑作为一个整体参与行为，语言和记忆等专门化的加工不是由特定的脑区完成，特定脑区的损伤并不引起特定的行为缺陷
  + 折中观点：尽管特定的神经区域负责某项独立的功能,但这些区域组成的网络以及他们之间的相互作用才是人类表现出的整体、综合行为的原因

+ 人工神经网络
  + 由一些简单的处理单元(神经元)组成，能够保存经验知识，并且能够利用这些经验知识完成一些任务通过“学习”来从环境中积累知识,知识被保存在神经元之间的连接上
  + 有两个方面类似于人脑:
    + 知识由网络通过学习过程得到;
    + 神经元间的互联,用来存储知识的已知的联合权值
  + 神经网络的特点：
    + 面向神经元和联结性
    + 并行、分布处理结构
    + 非线性，可以模拟任意的数学模型

+ 符号主义和联结主义：

  <img src="pics/image-20210102144828176.png" alt="image-20210102144828176" style="zoom:67%;" />

  

## 第二讲

### MP模型：

+ 输入$u=w_1x_1+\cdots+w_nx_n$，输出$y=f(\sum_{i=1}^nw_ix_i-\theta)$，$\theta$为阈值，当输入大于阈值时，神经元就兴奋。
+ 自变量及其函数的值、向量分量的值只取0和1函数、向量。

+ and神经元：

  <img src="pics/image-20210102155040460.png" alt="image-20210102155040460" style="zoom:67%;" />

+ or神经元：W = 2,threshold = 2

+ and-not与非神经元 (x1 AND NOT x2 -> y) : W = 2, p = 1, threshold = 2

+ XOR异或神经元 = (x1 AND NOT x2) OR (x2 AND NOT x1)，见作业



### 线性模型

+ 线性回归：$h(x)=wx+b$，最小化均方误差$E=\frac{1}{n}\sum_{i=1}^n(h(x_i)-y_i)^2$，求导可得闭式解。
+ 多元线性回归：$h(\mathbf{x})=\mathbf{w}^T\mathbf{x}+b=\mathbf{\hat{w}}^T\mathbf{x}$，均方误差为$E=\frac{1}{n}||\mathbf{y}-h(\mathbf{x})||^2$。求导得$2\mathbf{X}^T(\mathbf{X}\mathbf{\hat{w}}-\mathbf{y})=0$。

+ 对数线性回归：$h(x)=e^{w^Tx+b}$.
+ 线性二分类：$h(x)=f(wx+b)$，其中f为单位阶跃函数，将输出映射到{0, 1}。
  + 多分类问题，输出神经元可以直接输出一个向量，作为类别的one-hot编码，多个输出神经元。

### 感知器神经元

+ $y=f(\sum_j^n w_jy_j+b)=f(u+b)$，一个神经元接收其他神经元的输出$y_j$，再apply激活函数，输出y



## 第三讲

+ 学习神经元权重的方法：

  + 随机学习：随机更新权重矩阵和偏置矩阵，考虑输出和真实值的误差，如果小于阈值，则可以更新，否则再加上一个很小的随机数，再重新判定。其思想简单，实现容易，且具有能找到全局最优解等特点

  + Hebbian学习：

    <img src="pics/image-20210102174730874.png" alt="image-20210102174730874" style="zoom:67%;" />

    + 学习过程中，只有感知器对所有数据都进行了正确的分类权值才能移动：
      + 给定x, y，计算神经元输出t，计算误差E = t-y，如果是分类问题，此时分类正确就不用调整w，继续看下一个样本，否则：
      + $w_{new} = w_{old}+\beta*x*E$.



### 感知器学习算法

+ 梯度下降法：梯度的方向实际就是函数在此点上升最快的方向，而我们需要朝着下降最快的方向走，自然就是负的梯度的方向。
+ 学习率：太小可能导致迟迟走不到最低点或者无法跳出局部极小点；太大的话会导致错过最低点，无法稳定收敛。调整方法：
  + 学习率衰减、学习率预热、周期学习率、自适应调整学习率:AdaGrad,RMSprop,AdaDelta
  + 预热：在小批量梯度下降方法中，当批量大小设置的比较大时，通常需要较大的学习率，较大的学习率可能带来模型的不稳定，所以这两方面就发生了矛盾。
    + 在初始的几轮，采用较小的学习率，梯度下降到一定程度之后,再恢复到初始设置的学习率。
    + 逐渐预热：$\eta_t=\frac{t}{T}\eta_0$.
  + 困难：
    + 很难选择一个合适学习率
    + 对于所有的参数,均使用相同的学习率。不同参数的梯度大小有差异。
    + 很难跳出局部极值点，鞍点问题很难解决。

+ 两条定理表明误分类的次数k是有上界的，当训练数据集线性可分时，sign感知机学习算法迭代是收敛的。

### ADALINE

+ 激活函数为线性函数，试图最小化LMS

  <img src="pics/image-20210102182606950.png" alt="image-20210102182606950" style="zoom:67%;" />

  只要学习率满足$0<\eta<\frac{2}{\lambda_{max}}$，$\lambda_{max}$为输入向量自相关矩阵的最大特征值，LMS算法就是按方差收敛的

+ 和感知器的比较：

  <img src="pics/image-20210102182846410.png" alt="image-20210102182846410" style="zoom:67%;" />

  

## 第四讲

